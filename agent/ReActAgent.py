import re
from LLMClient import LLMClient
from ToolExecutor import ToolExecutor
from dotenv import load_dotenv
from SearchApi import search
from ToolExecutor import get_current_time
from constant import system_prompt

load_dotenv()

# ReAct æç¤ºè¯æ¨¡æ¿
REACT_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥åˆ©ç”¨å·¥å…·è§£å†³å¤æ‚é—®é¢˜ã€‚

## å¯ç”¨å·¥å…·
{tools}

## å›åº”æ ¼å¼
è¯·åŠ¡å¿…ä¸¥æ ¼éµå®ˆä»¥ä¸‹æ ¼å¼è¿›è¡Œå›åº”ï¼ˆä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å¤šä½™å†…å®¹ï¼‰ï¼š

Thought: æ€è€ƒå½“å‰çš„æƒ…å†µï¼Œåˆ†æé—®é¢˜ï¼Œå†³å®šä¸‹ä¸€æ­¥åšä»€ä¹ˆã€‚
Action: é‡‡å–çš„è¡ŒåŠ¨ï¼Œå¿…é¡»æ˜¯ä»¥ä¸‹æ ¼å¼ä¹‹ä¸€ï¼š
- `{{tool_name}}[{{tool_input}}]`: è°ƒç”¨å·¥å…·ã€‚ä¾‹å¦‚ `search[OpenAI 2025 models]`
- `Finish[æœ€ç»ˆç­”æ¡ˆ]`: ä»»åŠ¡å®Œæˆï¼Œè¿”å›æœ€ç»ˆç­”æ¡ˆã€‚

## ç¤ºä¾‹
Question: 2024å¹´å¥¥è¿ä¼šåœ¨å“ªé‡Œä¸¾åŠï¼Ÿ
Thought: æˆ‘éœ€è¦æœç´¢2024å¹´å¥¥è¿ä¼šçš„ä¸¾åŠåœ°ç‚¹ã€‚
Action: search[2024 olympics location]
Observation: 2024 Summer Olympics will be held in Paris, France.
Thought: æˆ‘å·²ç»è·å¾—äº†ç­”æ¡ˆï¼Œ2024å¹´å¥¥è¿ä¼šåœ¨å·´é»ä¸¾åŠã€‚
Action: Finish[2024å¹´å¥¥è¿ä¼šå°†åœ¨æ³•å›½å·´é»ä¸¾åŠã€‚]

## å½“å‰ä»»åŠ¡
Question: {question}
æé†’ï¼š{reflection}
History:
{history}

è¯·æ ¹æ® History ç»§ç»­æ€è€ƒã€‚å¦‚æœ History ä¸ºç©ºï¼Œè¯·å¼€å§‹ç¬¬ä¸€æ­¥æ€è€ƒã€‚
"""

class ReActAgent:
    """
    ReActä»£ç†ç±»ï¼Œå®ç°æ¨ç†+è¡ŒåŠ¨çš„å¾ªç¯ã€‚
    """
    def __init__(self, llmclient: LLMClient, tool_executor: ToolExecutor, max_step:int = 5):
        """
        åˆå§‹åŒ–ReActAgentã€‚

        Args:
            llmclient (LLMClient): LLMå®¢æˆ·ç«¯å®ä¾‹ã€‚
            tool_executor (ToolExecutor): å·¥å…·æ‰§è¡Œå™¨å®ä¾‹ã€‚
            max_step (int): æœ€å¤§æ€è€ƒæ­¥æ•°ï¼Œé»˜è®¤ä¸º5ã€‚
        """
        self.llmclient = llmclient
        self.tool_executor = tool_executor
        self.max_step = max_step
    
    def _parse_response(self, response_text: str):
        """
        è§£æLLMçš„å“åº”ï¼Œæå–Thoughtå’ŒActionã€‚

        Args:
            response_text (str): LLMçš„å“åº”æ–‡æœ¬ã€‚

        Returns:
            tuple: (thought, action) å­—ç¬¦ä¸²å…ƒç»„ã€‚
        """
        thought_match = re.search(r"Thought: (.*)", response_text, re.DOTALL)
        action_match = re.search(r"Action: (.*)", response_text, re.DOTALL)
        thought = thought_match.group(1).strip() if thought_match else None
        action = action_match.group(1).strip() if action_match else None
        return thought, action

    def _parse_action(self, action: str):
        """
        è§£æActionå­—ç¬¦ä¸²ï¼Œæå–å·¥å…·åç§°å’Œè¾“å…¥ã€‚

        Args:
            action (str): Actionå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º tool_name[tool_input]ã€‚

        Returns:
            tuple: (tool_name, tool_input) å­—ç¬¦ä¸²å…ƒç»„ã€‚
        """
        match = re.match(r"(\w+)\[(.*)\]", action, re.DOTALL)
        if match:
            return match.group(1), match.group(2)
        return None, None

    def run(self, question: str, reflection: str = ""):
        """
        è¿è¡ŒReActå¾ªç¯è§£å†³é—®é¢˜ã€‚

        Args:
            question (str): ç”¨æˆ·çš„é—®é¢˜ã€‚
            reflection (str): å¯é€‰çš„åæ€å†…å®¹ï¼Œç”¨äºæç¤ºLLMã€‚

        Returns:
            str: æœ€ç»ˆç­”æ¡ˆã€‚å¦‚æœæœªæ‰¾åˆ°ç­”æ¡ˆæˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œè¿”å›Noneã€‚
        """
        self.history = []
        
        current_step = 0
        while current_step < self.max_step:
            current_step += 1
            print(f"\nå½“å‰æ­¥æ•°: {current_step}")

            tools = self.tool_executor.get_tools()
            # æ ¼å¼åŒ–æç¤ºè¯
            prompt = REACT_PROMPT_TEMPLATE.format(
                tools=tools,
                question=question,
                history="\n".join(self.history),
                reflection=reflection
            )
            #print(f"Prompt:\n{prompt}")
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
            # è°ƒç”¨LLMè¿›è¡Œæ€è€ƒ
            response_text = self.llmclient.think(messages)
            print(response_text)
            if not response_text:
                print("LLM å“åº”å¤±è´¥")
                break
            
            # è§£æå“åº”
            thought, action = self._parse_response(response_text)
            if not action:
                print("æœªè·å¾—ä¸‹ä¸€æ­¥action, æ­¥éª¤ç»“æŸ")
                break

            # æ£€æŸ¥æ˜¯å¦å®Œæˆä»»åŠ¡
            if action.startswith("Finish"):
                final_answer_match = re.match(r"Finish\[(.*)\]", action, re.DOTALL)
                if final_answer_match:
                    final_answer = final_answer_match.group(1)
                    print(f"ğŸ‰ æœ€ç»ˆç­”æ¡ˆ: {final_answer}")
                    return final_answer
                else:
                    print("æ— æ³•è§£æ Finish action")
                    break
            
            # è§£æå¹¶æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_name, tool_input = self._parse_action(action)
            if not tool_name:
                print("LLM å“åº”æ ¼å¼é”™è¯¯")
                break
            
            tool_function = self.tool_executor.get_tool(tool_name)
            if not tool_function:
                observation = f"å·¥å…· '{tool_name}' ä¸å­˜åœ¨"
            else:
                observation = tool_function(tool_input)
            print(f"è§‚å¯Ÿç»“æœ: {observation}")
            
            # æ›´æ–°å†å²è®°å½•
            self.history.append(f"Thought: {thought}\nAction: {action}")
            self.history.append(f"Observation: {observation}")

        print("å·²ç»è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œæµç¨‹ç»ˆæ­¢")
        return None
        

if __name__ == "__main__":
    try:
        llmclient = LLMClient()
        tool_executor = ToolExecutor()
        tool_executor.register_tool("search", "ä¸€ä¸ªç½‘é¡µæœç´¢ï¼Œå¯ä»¥æœç´¢ä¿¡æ¯", search)
        tool_executor.register_tool("get_current_time", "è·å–å½“å‰æ—¶é—´", get_current_time)

        agent = ReActAgent(llmclient, tool_executor)
        question = "2025å¹´OpenAIå‘å¸ƒè¿‡çš„æ¨¡å‹éƒ½æœ‰ä»€ä¹ˆï¼Ÿ"
        answer = agent.run(question)
        print("\næœ€ç»ˆç­”æ¡ˆ:", answer)
    except Exception as e:
        print(f"Exception: {e}")
